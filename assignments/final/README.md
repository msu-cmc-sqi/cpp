# Финальное домашнее задание: Гибридный AI-агент (Hurated API + локальная модель)

## Цели

* Довести своего агента (из урока 04) до **“продуктового” состояния**.
* Реализовать **два источника инференса**:

  * удалённый API: **Hurated API**;
  * **локальная модель** (GGUF через `llama.cpp` / `llama-cpp-python` или аналогичный стек).
* Научиться **переключать тип модели** через **конфигурационный файл**.
* Подготовить понятное **описание проекта, инструкцию по сборке/запуску и примеры работы**.
* Оформить всё в виде **финального pull request’а**.

---

## Общее описание задания

У вас уже есть базовый агент на основе `ai_agent_example` (урок 04) и опыт работы с локальными моделями (урок 09). Теперь нужно:

1. Сделать **единый CLI-агент**, который:

   * умеет работать **через Hurated API**;
   * умеет работать **через локальную модель**;
   * позволяет выбирать режим **через `config.json`** (без перекомпиляции).

2. Сохранить (и при необходимости доработать) **функциональность**, которую вы спроектировали ранее:

   * проверка кода;
   * резюмирование текста;
   * цепочки запросов (поддержка истории запросов);
   * CLI-помощник;
   * или ваша собственная идея.

3. Оформить всё так, чтобы **любой человек** мог:

   * клонировать репозиторий;
   * собрать проект;
   * настроить конфиг;
   * запустить примеры и получить такой же результат, как в README.

---

## Требования к функциональности

### 1. Два режима работы

Агент должен уметь работать минимум в двух режимах инференса:

1. **Hurated API**

   * Используем уже знакомый подход: SSL, HTTP(S), JSON, запрос к `ai-api.hurated.com`.
   * Ключ берём из `config.json` (не хардкодим в коде!).

2. **Локальная модель**
   Варианты (выберите один, но можно и комбинировать):

   * **C++ + `llama.cpp`**:

     * линковка к библиотеке / запуск через CLI `llama-cli` / `main` из `llama.cpp`;
     * агент передаёт промпт и получает ответ от локальной модели.
   * **Запуск через отдельный процесс**:

     * агент на C++ вызывает Python-скрипт или бинарник с локальной моделью;
     * общение через stdio/файл/локальный HTTP-сервис.
   * **`llama-cpp-python` + небольшой HTTP-сервер**:

     * локальный сервер, поднятый отдельно, а ваш агент бьётся к нему как к обычному API.

Главное: **локальная модель действительно используется**, а не “заглушка с printf”.

### 2. Переключение модели через конфиг

В `config.json` должно быть **минимум**:

```json
{
  "mode": "hurated",   // или "local"
  "hurated": {
    "host": "ai-api.hurated.com",
    "port": "443",
    "api_key": "<ваш ключ>"
  },
  "local": {
    "backend": "llama.cpp",
    "model_path": "./models/your-model.gguf",
    "context_length": 2048
  }
}
```

*Имя полей и структура могут отличаться, но идея такая: один файл конфигурации, в котором можно переключить режим без изменения кода.*

Агент при запуске:

1. Читает `config.json`.
2. Понимает, какой режим (`mode`) выбран.
3. Инициализирует **соответствующий бэкенд** (Hurated или локальный).
4. Работает дальше в выбранном режиме.

### 3. Единый интерфейс агента

* В идеале **одна и та же CLI-команда** работает и с Hurated, и с локальной моделью — различие лишь в конфиге.
* Примеры:

  ```bash
  ./ai_agent --mode explain --file examples/code.cpp
  ./ai_agent --mode summarize --file examples/text.txt
  ./ai_agent --interactive
  ```
* В README чётко описано:

  * какие режимы есть;
  * какие параметры принимает программа;
  * что делает каждый режим.

---

## Требования к коду и проекту

* База: **C++17**, сборка через **CMake**.
* Не ломаем поддержку **macOS / Linux** (по возможности).
* Не хардкодим ключи/пути к моделям — всё в `config.json`.
* Работа с JSON — через `nlohmann/json` (как в примере).
* Ошибки сети/парсинга/модели должны:

  * корректно обрабатываться;
  * выводить понятное сообщение пользователю.
* Код компилируется **без ошибок**, варнинги по возможности исправлены.
* Структура репозитория:

  * `assignments/final/<фамилия_студента>/...`
  * Внутри — исходники, CMakeLists, ваш README, примеры, конфиг.

---

## README: что обязательно должно быть

Файл: `assignments/final/<фамилия>/README.md`

### 1. Название и идея

```markdown
# <Название вашего агента>

## Идея
Коротко: какую задачу решает агент, для кого и зачем.
```

Опишите:

* что делает агент;
* для какой аудитории;
* чем он полезен.

### 2. Возможности

```markdown
## Возможности
- Пункт 1 (например: анализ кода C++)
- Пункт 2 (например: суммаризация длинных текстов)
- Пункт 3 (например: режим интерактивного чата)
```

### 3. Установка и сборка

Пример (можно адаптировать под ваш репозиторий):

````markdown
## Установка и сборка

```bash
git clone <ваш-репозиторий>
cd <путь_до>/assignments/final/<фамилия>

mkdir build && cd build
cmake ..
make -j
````

````

Если нужны дополнительные зависимости (например, `llama.cpp` как подмодуль/библиотека) — **обязательно** опишите, как их установить/собрать.

### 4. Настройка `config.json`

Пример раздела:

```markdown
## Настройка

В корне проекта создайте файл `config.json`:

```json
{
  "mode": "hurated",
  "hurated": {
    "host": "ai-api.hurated.com",
    "port": "443",
    "api_key": "<ваш ключ>"
  },
    "local": {
    "backend": "llama.cpp",
    "model_path": "./models/your-model.gguf",
    "context_length": 2048
  }
}
````

* `mode` — `"hurated"` или `"local"`.
* Для `hurated`:

  * `api_key` — ваш ключ (не коммитим в репозиторий!).
* Для `local`:

  * `model_path` — путь к GGUF-модели;
  * дополнительные параметры (context_length, temperature и т.д. по вашему желанию).

````

Если требуется отдельная установка моделей, опишите:

```markdown
### Установка локальной модели

1. Скачайте модель в формате GGUF и положите в папку `models/`.
2. Убедитесь, что путь в `config.json` (`local.model_path`) совпадает с реальным.
3. Если вы используете `llama.cpp`:
   - Установите зависимости (…).
   - Сборка `llama.cpp` (…).
````

### 5. Примеры запуска

Обязательный раздел с конкретными командами и реальными примерами вывода.

````markdown
## Примеры запуска

### Режим Hurated

```bash
# Пример: проверка кода
./ai_agent --mode check --file examples/code.cpp
````

Ожидаемый (сокращённый) вывод:

```text
[backend] hurated
[info] Анализ кода из examples/code.cpp ...
[result]
- Найдены возможные ошибки:
  1) ...
  2) ...
```

### Режим локальной модели

```bash
./ai_agent --mode check --file examples/code.cpp
```

Ожидаемый вывод:

```text
[backend] local (llama.cpp)
[info] Анализ кода из examples/code.cpp ...
[result]
- Рекомендации:
  1) ...
  2) ...
```

### Интерактивный режим

```bash
./ai_agent --interactive
```

Пример диалога:

```text
You: Привет, оцени, пожалуйста, этот фрагмент кода из file.cpp
Agent: ...
You: Какие тесты можно добавить?
Agent: ...
```

````

Важно: **примеры должны соответствовать реальному поведению программы**.

### 6. Примеры входных данных

Сделайте небольшую папку с тестовыми данными:

* `examples/code.cpp` — пример кода для анализа;
* `examples/text.txt` — текст для суммаризации;
* `examples/query.txt` — пример запроса и т.п.

Опишите их в README:

```markdown
## Тестовые файлы

- `examples/code.cpp` — простой пример кода с ошибками/особенностями.
- `examples/text.txt` — длинный текст для суммаризации.
````

---

## Что именно нужно сдать

1. **Код проекта** в папке:

   `assignments/final/<фамилия>/`

   Там же:

   * `CMakeLists.txt`;
   * исходники (`src/`, `include/` и др.);
   * `README.md`;
   * пример `config.json` (без реальных ключей);
   * папка `examples/` с тестовыми файлами.

2. **Рабочий README**:

   * описание идеи и возможностей;
   * детальная инструкция по сборке;
   * настройка `config.json`;
   * примеры запуска (Hurated + локальная модель);
   * примеры входных данных.

3. **Pull Request** в общий репозиторий курса:

   * ветка: по договорённости (например, `final/<фамилия>`).
   * PR должен содержать только вашу папку `assignments/final/<фамилия>/` и необходимые изменения верхнего CMake, если они есть.

4. **Краткая устная презентация** на занятии:

   * 3–5 минут;
   * что делает ваш агент;
   * как устроено переключение backend’ов;
   * покажите краткое демо (скриншоты/терминал).

---

## Дополнительные (необязательные, но приветствуются) улучшения

* Логирование в файл (с указанием, какой бэкенд использовался).
* Метрики: время ответа локальной модели vs Hurated.
* Конфигурируемые параметры генерации (temperature, top_p).
* Поддержка нескольких локальных моделей с выбором через конфиг.
* Небольшие автотесты (хотя бы для парсинга конфига и CLI-аргументов).
* Выбор режима mode через аргумент командной строки


